{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bhAvqNYzb_X2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import scipy.optimize as optim\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LFR(self, X, y, k):\n",
        "  def __init__():\n",
        "    \"\"\"Here we will instantiate all of our parameters.\n",
        "    Learnable parameters: V (our prototype space), w (the weights associated with y_hat), and alpha (coefficients for distance)\n",
        "    non-learnable parameters: M_nk, M_k\"\"\"\n",
        "\n",
        "    #TODO: will need to initialize & fill in dimensions accordingly \n",
        "\n",
        "    #learnable parameters --> make sure to turn into torch.Parameters()!!\n",
        "    self.v = torch.tensor()\n",
        "    self.w = torch.tensor()\n",
        "    self.alpha = torch.tensor()\n",
        "\n",
        "    #unlearnable parameters\n",
        "    \n",
        "    #keep record of sensitive vs non-sensitive coefficients\n",
        "    self.alpha_S = torch.tensor()\n",
        "    self.alpha_nS = torch.tensor()\n",
        "\n",
        "    self.distances_S = torch.tensor()\n",
        "    self.distances_nS = torch.tensor()\n",
        "\n",
        "    self.M_nk_S = torch.tensor()\n",
        "    self.M_nk_nS = torch.tensor()\n",
        "\n",
        "    self.M_k_S = torch.tensor()\n",
        "    self.M_k_nS = torch.tensor()\n",
        "\n",
        "    self.distances = torch.tensor()\n",
        "    self.M_nk = torch.tensor()\n",
        "    self.M_k = torch.tensor()\n",
        "\n",
        "  def x_n_hat(X, M_nk, v, N, P, k):\n",
        "    x_n_hat = np.zeros((N, P))\n",
        "    L_x = 0.0\n",
        "    for i in range(N):\n",
        "        for p in range(P):\n",
        "            for j in range(k):\n",
        "                x_n_hat[i, p] += M_nk[i, j] * v[j, p]\n",
        "            L_x += (X[i, p] - x_n_hat[i, p]) * (X[i, p] - x_n_hat[i, p])\n",
        "    return x_n_hat, L_x\n",
        "  \n",
        "  def yhat(M_nk, y, w, N, k):\n",
        "    yhat = np.zeros(N)\n",
        "    L_y = 0.0\n",
        "    for i in range(N):\n",
        "        for j in range(k):\n",
        "            yhat[i] += M_nk[i, j] * w[j]\n",
        "        yhat[i] = 1e-6 if yhat[i] <= 0 else yhat[i]\n",
        "        yhat[i] = 0.999 if yhat[i] >= 1 else yhat[i]\n",
        "        L_y += -1 * y[i] * np.log(yhat[i]) - (1.0 - y[i]) * np.log(1.0 - yhat[i])\n",
        "    return yhat, L_y\n",
        "  \n",
        "  def recover_Z():\n",
        "    #TODO: fill in whatever we need to calculate LZ\n",
        "    pass\n",
        "\n",
        "  def forward(self,X,y):\n",
        "    #TODO: make sure this works accordingly! do we need to readjust our parameters? do we need to call self?\n",
        "    x_n_hat, L_x = x_n_hat(X, M_nk, v, N, P, k)\n",
        "    y_hat, L_y = yhat(M_nk, y, w, N, k)\n",
        "    Z = recover_Z()\n",
        "    return x_n_hat, L_x, y_hat, L_y, Z\n",
        "  \n",
        "  def loss_fn(self, X, y, x_n_hat, L_x, y_hat, L_y, Z):\n",
        "    #TODO: calculate L = Lx + Ly + Lz\n",
        "    pass\n",
        "  \n",
        "  def update_M_nk():\n",
        "    #TODO: update M_nk after SGD\n",
        "    pass\n",
        "  \n",
        "  def update_Mk():\n",
        "    #TODO: update M_k after SGD\n",
        "    pass\n",
        "\n",
        "  def update_distances():\n",
        "    #TODO: update distances\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "joL2QOlScSpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LFR()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        x_n_hat, L_x, y_hat, L_y, Z = model.forward(X)\n",
        "        loss = model.loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.update_Mnk()\n",
        "        model.update_Mk()\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "vzGAA2l1gBEK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}